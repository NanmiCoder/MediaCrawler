# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  

import asyncio
import json
import os
import subprocess
import sys
import tempfile
import threading
import time
import uuid
import argparse
from contextlib import asynccontextmanager
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, field_validator
from fastapi_offline import FastAPIOffline
import uvicorn

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥é…ç½®
try:
    from config import base_config
    from config import db_config
    print("âœ… é…ç½®æ–‡ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶: {e}")
    print("å°†ä½¿ç”¨é»˜è®¤é…ç½®")
    base_config = None
    db_config = None

# å…¨å±€å˜é‡
tasks: Dict[str, 'TaskInfo'] = {}  # ä»»åŠ¡å­˜å‚¨
running_processes: Dict[str, subprocess.Popen] = {}  # è¿è¡Œä¸­çš„è¿›ç¨‹

# æœåŠ¡å™¨é…ç½®
class ServerConfig:
    """æœåŠ¡å™¨é…ç½®ç±»"""
    def __init__(self):
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        self.host: str = os.getenv("API_HOST", "0.0.0.0")
        self.port: int = int(os.getenv("API_PORT", "8000"))
        self.reload: bool = os.getenv("API_RELOAD", "true").lower() == "true"
        self.debug: bool = os.getenv("API_DEBUG", "false").lower() == "true"
        
    def get_api_docs_url(self) -> str:
        """è·å–APIæ–‡æ¡£URL"""
        return f"http://localhost:{self.port}/docs"
        
    def get_redoc_url(self) -> str:
        """è·å–ReDocæ–‡æ¡£URL"""
        return f"http://localhost:{self.port}/redoc"
        
    def get_server_url(self) -> str:
        """è·å–æœåŠ¡å™¨URL"""
        return f"http://{self.host}:{self.port}"
        
    def update_port(self, port: int) -> None:
        """åŠ¨æ€æ›´æ–°ç«¯å£å·"""
        if not isinstance(port, int) or port < 1 or port > 65535:
            raise ValueError("ç«¯å£å·å¿…é¡»æ˜¯1-65535ä¹‹é—´çš„æ•´æ•°")
        self.port = port
        
    def get_config_info(self) -> Dict[str, Any]:
        """è·å–é…ç½®ä¿¡æ¯"""
        return {
            "host": self.host,
            "port": self.port,
            "reload": self.reload,
            "debug": self.debug,
            "api_docs_url": self.get_api_docs_url(),
            "redoc_url": self.get_redoc_url(),
            "server_url": self.get_server_url()
        }
        
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"ServerConfig(host={self.host}, port={self.port}, reload={self.reload}, debug={self.debug})"

# å…¨å±€æœåŠ¡å™¨é…ç½®å®ä¾‹
server_config = ServerConfig()

def update_server_config(host: Optional[str] = None, port: Optional[int] = None, 
                        reload: Optional[bool] = None, debug: Optional[bool] = None):
    """æ›´æ–°å…¨å±€æœåŠ¡å™¨é…ç½®"""
    if host is not None:
        server_config.host = host
    if port is not None:
        server_config.update_port(port)
    if reload is not None:
        server_config.reload = reload
    if debug is not None:
        server_config.debug = debug

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨äº‹ä»¶
    print("="*60)
    print("MediaCrawler API æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ!")
    print("="*60)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"æœåŠ¡å™¨é…ç½®: {server_config}")
    print("-"*40)
    print("ğŸ“š æ–‡æ¡£åœ°å€:")
    print(f"  â€¢ Swagger UI: {server_config.get_api_docs_url()}")
    print(f"  â€¢ ReDoc:      {server_config.get_redoc_url()}")
    print(f"ğŸŒ æœåŠ¡å™¨åœ°å€: {server_config.get_server_url()}")
    print("="*60)
    
    if server_config.debug:
        print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
        config_info = server_config.get_config_info()
        print("è¯¦ç»†é…ç½®ä¿¡æ¯:")
        for key, value in config_info.items():
            print(f"  {key}: {value}")
        print("-"*40)
    
    yield
    
    # å…³é—­äº‹ä»¶
    print("æ­£åœ¨å…³é—­MediaCrawler APIæœåŠ¡å™¨...")
    
    # ç»ˆæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„è¿›ç¨‹
    for task_id, process in running_processes.items():
        try:
            process.terminate()
            process.wait(timeout=5)
        except:
            try:
                process.kill()
            except:
                pass
    
    print("MediaCrawler APIæœåŠ¡å™¨å·²å…³é—­")

def create_app() -> FastAPIOffline:
    """åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹"""
    return FastAPIOffline(
        title="MediaCrawler API",
        description="MediaCrawlerå¼‚æ­¥åç«¯APIæœåŠ¡å™¨ - å¤šå¹³å°è‡ªåª’ä½“æ•°æ®é‡‡é›†å·¥å…·",
        version="1.0.0",
        docs_url="/docs",
        redoc_url="/redoc",
        lifespan=lifespan
    )

# åˆ›å»ºFastAPIåº”ç”¨ - ä½¿ç”¨fastapi-offlineæ”¯æŒç¦»çº¿æ–‡æ¡£
app = create_app()

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æšä¸¾å®šä¹‰
class PlatformEnum(str, Enum):
    """æ”¯æŒçš„å¹³å°æšä¸¾"""
    XHS = "xhs"  # å°çº¢ä¹¦
    DY = "dy"    # æŠ–éŸ³
    KS = "ks"    # å¿«æ‰‹
    BILI = "bili"  # Bç«™
    WB = "wb"    # å¾®åš
    TIEBA = "tieba"  # è´´å§
    ZHIHU = "zhihu"  # çŸ¥ä¹

class LoginTypeEnum(str, Enum):
    """ç™»å½•ç±»å‹æšä¸¾"""
    QRCODE = "qrcode"  # äºŒç»´ç ç™»å½•
    PHONE = "phone"    # æ‰‹æœºå·ç™»å½•
    COOKIE = "cookie"  # Cookieç™»å½•

class CrawlerTypeEnum(str, Enum):
    """çˆ¬å–ç±»å‹æšä¸¾"""
    SEARCH = "search"    # å…³é”®è¯æœç´¢
    DETAIL = "detail"    # æŒ‡å®šå¸–å­ID
    CREATOR = "creator"  # æŒ‡å®šåˆ›ä½œè€…ä¸»é¡µ

class SaveDataOptionEnum(str, Enum):
    """æ•°æ®ä¿å­˜é€‰é¡¹æšä¸¾"""
    CSV = "csv"
    DB = "db"
    JSON = "json"

class TaskStatusEnum(str, Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"    # ç­‰å¾…ä¸­
    RUNNING = "running"    # è¿è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"      # å¤±è´¥
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ

# è¯·æ±‚æ¨¡å‹
class CrawlerRequest(BaseModel):
    """çˆ¬è™«è¯·æ±‚æ¨¡å‹"""
    platform: PlatformEnum = Field(..., description="å¹³å°é€‰æ‹©")
    login_type: LoginTypeEnum = Field(default=LoginTypeEnum.QRCODE, description="ç™»å½•ç±»å‹")
    crawler_type: CrawlerTypeEnum = Field(default=CrawlerTypeEnum.SEARCH, description="çˆ¬å–ç±»å‹")
    start_page: int = Field(default=1, ge=1, description="èµ·å§‹é¡µç ")
    keywords: Optional[str] = Field(default="", description="æœç´¢å…³é”®è¯")
    get_comment: bool = Field(default=False, description="æ˜¯å¦è·å–ä¸€çº§è¯„è®º")
    get_sub_comment: bool = Field(default=False, description="æ˜¯å¦è·å–äºŒçº§è¯„è®º")
    save_data_option: SaveDataOptionEnum = Field(default=SaveDataOptionEnum.JSON, description="æ•°æ®ä¿å­˜é€‰é¡¹")
    cookies: Optional[str] = Field(default="", description="Cookieå­—ç¬¦ä¸²")
    
    @field_validator('keywords', mode='after')
    @classmethod
    def validate_keywords(cls, v):
        """éªŒè¯å…³é”®è¯"""
        # æ³¨æ„ï¼šåœ¨Pydantic V2ä¸­ï¼Œå•å­—æ®µéªŒè¯å™¨æ— æ³•ç›´æ¥è®¿é—®å…¶ä»–å­—æ®µ
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–éªŒè¯é€»è¾‘ï¼Œæˆ–è€…å¯ä»¥ä½¿ç”¨model_validatorè¿›è¡Œæ•´ä½“éªŒè¯
        return v
    
    def model_post_init(self, __context) -> None:
        """æ¨¡å‹åˆå§‹åŒ–åéªŒè¯"""
        if self.crawler_type == CrawlerTypeEnum.SEARCH and not self.keywords:
            raise ValueError('æœç´¢ç±»å‹å¿…é¡»æä¾›å…³é”®è¯')
        super().model_post_init(__context) if hasattr(super(), 'model_post_init') else None

# å“åº”æ¨¡å‹
class TaskInfo(BaseModel):
    """ä»»åŠ¡ä¿¡æ¯æ¨¡å‹"""
    task_id: str
    status: TaskStatusEnum
    platform: str
    crawler_type: str
    keywords: Optional[str] = None
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    result_data: Optional[Dict[str, Any]] = None

class BaseResponse(BaseModel):
    """åŸºç¡€å“åº”æ¨¡å‹"""
    success: bool
    message: str
    timestamp: datetime = Field(default_factory=datetime.now)

class CrawlerResponse(BaseResponse):
    """çˆ¬è™«å“åº”æ¨¡å‹"""
    data: Optional[Dict[str, Any]] = None
    task_id: Optional[str] = None

class TaskListResponse(BaseResponse):
    """ä»»åŠ¡åˆ—è¡¨å“åº”æ¨¡å‹"""
    data: List[TaskInfo]
    total: int

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str = "healthy"
    timestamp: datetime = Field(default_factory=datetime.now)
    version: str = "1.0.0"



def str2bool(v: str) -> bool:
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise ValueError('Boolean value expected.')

def build_command(request: CrawlerRequest) -> List[str]:
    """æ„å»ºçˆ¬è™«å‘½ä»¤"""
    cmd = [sys.executable, "main.py"]
    
    # åŸºç¡€å‚æ•°
    cmd.extend(["--platform", request.platform])
    cmd.extend(["--lt", request.login_type])
    cmd.extend(["--type", request.crawler_type])
    
    # å…³é”®è¯å‚æ•°
    if request.keywords:
        cmd.extend(["--keywords", request.keywords])
    
    # å¯é€‰å‚æ•°
    if request.start_page is not None:
        cmd.extend(["--start", str(request.start_page)])
    
    if request.get_comment is not None:
        cmd.extend(["--get_comment", str(request.get_comment).lower()])
    
    if request.get_sub_comment is not None:
        cmd.extend(["--get_sub_comment", str(request.get_sub_comment).lower()])
    
    if request.save_data_option:
        cmd.extend(["--save", request.save_data_option])
    
    if request.cookies:
        cmd.extend(["--cookies", request.cookies])
    
    return cmd

def run_crawler_subprocess(task_id: str, command: List[str]):
    """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçˆ¬è™«"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        if task_id in tasks:
            tasks[task_id].status = TaskStatusEnum.RUNNING
            tasks[task_id].started_at = datetime.now()
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        # å¯åŠ¨å­è¿›ç¨‹
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8',
            cwd=str(project_root),
            env=env
        )
        
        # ä¿å­˜è¿›ç¨‹å¼•ç”¨
        running_processes[task_id] = process
        
        # ç­‰å¾…è¿›ç¨‹å®Œæˆ
        stdout, stderr = process.communicate()
        
        # ç§»é™¤è¿›ç¨‹å¼•ç”¨
        if task_id in running_processes:
            del running_processes[task_id]
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id in tasks:
            tasks[task_id].completed_at = datetime.now()
            
            if process.returncode == 0:
                tasks[task_id].status = TaskStatusEnum.COMPLETED
                tasks[task_id].result_data = {
                    "stdout": stdout,
                    "stderr": stderr,
                    "return_code": process.returncode
                }
            else:
                tasks[task_id].status = TaskStatusEnum.FAILED
                tasks[task_id].error_message = stderr or "è¿›ç¨‹å¼‚å¸¸é€€å‡º"
                tasks[task_id].result_data = {
                    "stdout": stdout,
                    "stderr": stderr,
                    "return_code": process.returncode
                }
    
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        if task_id in tasks:
            tasks[task_id].status = TaskStatusEnum.FAILED
            tasks[task_id].error_message = str(e)
            tasks[task_id].completed_at = datetime.now()
        
        # æ¸…ç†è¿›ç¨‹å¼•ç”¨
        if task_id in running_processes:
            del running_processes[task_id]

# APIè·¯ç”±
@app.get("/", response_model=HealthResponse)
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡çŠ¶æ€"""
    return HealthResponse()

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return HealthResponse()

@app.post("/crawler/run", response_model=CrawlerResponse)
async def run_crawler_sync(request: CrawlerRequest):
    """åŒæ­¥è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
        task_info = TaskInfo(
            task_id=task_id,
            status=TaskStatusEnum.PENDING,
            platform=request.platform.value,
            crawler_type=request.crawler_type.value,
            keywords=request.keywords,
            created_at=datetime.now()
        )
        
        # ä¿å­˜ä»»åŠ¡
        tasks[task_id] = task_info
        
        # æ„å»ºå‘½ä»¤
        command = build_command(request)
        
        # åŒæ­¥æ‰§è¡Œçˆ¬è™«
        run_crawler_subprocess(task_id, command)
        
        # è·å–æœ€æ–°ä»»åŠ¡çŠ¶æ€
        final_task = tasks[task_id]
        
        if final_task.status == TaskStatusEnum.COMPLETED:
            return CrawlerResponse(
                success=True,
                message="çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ",
                task_id=task_id,
                data=final_task.result_data
            )
        else:
            return CrawlerResponse(
                success=False,
                message=f"çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {final_task.error_message}",
                task_id=task_id,
                data=final_task.result_data
            )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")

@app.post("/crawler/run-async", response_model=CrawlerResponse)
async def run_crawler_async(request: CrawlerRequest, background_tasks: BackgroundTasks):
    """å¼‚æ­¥è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
        task_info = TaskInfo(
            task_id=task_id,
            status=TaskStatusEnum.PENDING,
            platform=request.platform.value,
            crawler_type=request.crawler_type.value,
            keywords=request.keywords,
            created_at=datetime.now()
        )
        
        # ä¿å­˜ä»»åŠ¡
        tasks[task_id] = task_info
        
        # æ„å»ºå‘½ä»¤
        command = build_command(request)
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawler_subprocess, task_id, command)
        
        return CrawlerResponse(
            success=True,
            message="çˆ¬è™«ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°æ‰§è¡Œ",
            task_id=task_id
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")

@app.get("/crawler/task/{task_id}", response_model=CrawlerResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    return CrawlerResponse(
        success=True,
        message="è·å–ä»»åŠ¡çŠ¶æ€æˆåŠŸ",
        task_id=task_id,
        data=task.dict()
    )

@app.get("/crawler/tasks", response_model=TaskListResponse)
async def get_all_tasks(limit: int = 50, offset: int = 0):
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    all_tasks = list(tasks.values())
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    all_tasks.sort(key=lambda x: x.created_at, reverse=True)
    
    # åˆ†é¡µ
    paginated_tasks = all_tasks[offset:offset + limit]
    
    return TaskListResponse(
        success=True,
        message="è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
        data=paginated_tasks,
        total=len(all_tasks)
    )

@app.delete("/crawler/task/{task_id}", response_model=BaseResponse)
async def cancel_task(task_id: str):
    """å–æ¶ˆ/åˆ é™¤ä»»åŠ¡"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    
    # å¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œå°è¯•ç»ˆæ­¢è¿›ç¨‹
    if task.status == TaskStatusEnum.RUNNING and task_id in running_processes:
        try:
            process = running_processes[task_id]
            process.terminate()
            # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()  # å¼ºåˆ¶æ€æ­»è¿›ç¨‹
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = TaskStatusEnum.CANCELLED
            task.completed_at = datetime.now()
            task.error_message = "ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ"
            
            # æ¸…ç†è¿›ç¨‹å¼•ç”¨
            if task_id in running_processes:
                del running_processes[task_id]
            
            return BaseResponse(success=True, message="ä»»åŠ¡å·²å–æ¶ˆ")
        except Exception as e:
            return BaseResponse(success=False, message=f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}")
    
    # åˆ é™¤ä»»åŠ¡è®°å½•
    del tasks[task_id]
    
    return BaseResponse(success=True, message="ä»»åŠ¡å·²åˆ é™¤")

@app.get("/crawler/platforms")
async def get_supported_platforms():
    """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
    platforms = [
        {"value": "xhs", "label": "å°çº¢ä¹¦", "description": "å°çº¢ä¹¦å¹³å°æ•°æ®é‡‡é›†"},
        {"value": "dy", "label": "æŠ–éŸ³", "description": "æŠ–éŸ³å¹³å°æ•°æ®é‡‡é›†"},
        {"value": "ks", "label": "å¿«æ‰‹", "description": "å¿«æ‰‹å¹³å°æ•°æ®é‡‡é›†"},
        {"value": "bili", "label": "Bç«™", "description": "å“”å“©å“”å“©å¹³å°æ•°æ®é‡‡é›†"},
        {"value": "wb", "label": "å¾®åš", "description": "å¾®åšå¹³å°æ•°æ®é‡‡é›†"},
        {"value": "tieba", "label": "è´´å§", "description": "ç™¾åº¦è´´å§å¹³å°æ•°æ®é‡‡é›†"},
        {"value": "zhihu", "label": "çŸ¥ä¹", "description": "çŸ¥ä¹å¹³å°æ•°æ®é‡‡é›†"}
    ]
    
    return {
        "success": True,
        "message": "è·å–æ”¯æŒå¹³å°åˆ—è¡¨æˆåŠŸ",
        "data": platforms
    }

# å¼‚å¸¸å¤„ç†
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "message": exc.detail,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(exc)}",
            "timestamp": datetime.now().isoformat()
        }
    )



def start_server(host: Optional[str] = None, port: Optional[int] = None, 
                reload: Optional[bool] = None, debug: Optional[bool] = None):
    """å¯åŠ¨æœåŠ¡å™¨çš„å‡½æ•°
    
    Args:
        host: æœåŠ¡å™¨ä¸»æœºåœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        port: æœåŠ¡å™¨ç«¯å£å·ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        reload: æ˜¯å¦å¯ç”¨çƒ­é‡è½½ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
    """
    # åœ¨å¯åŠ¨å‰æ›´æ–°å…¨å±€é…ç½®ï¼Œç¡®ä¿å¯åŠ¨äº‹ä»¶èƒ½è·å–åˆ°æ­£ç¡®é…ç½®
    update_server_config(host=host, port=port, reload=reload, debug=debug)
    
    # æ˜¾ç¤ºå¯åŠ¨å‰çš„é…ç½®ä¿¡æ¯
    print(f"ğŸš€ å‡†å¤‡å¯åŠ¨æœåŠ¡å™¨ï¼Œé…ç½®: {server_config}")
    
    # å¯åŠ¨æœåŠ¡å™¨
    try:
        if server_config.reload:
            # çƒ­é‡è½½æ¨¡å¼ï¼šä½¿ç”¨æ¨¡å—å­—ç¬¦ä¸²
            uvicorn.run(
                "api:app",
                host=server_config.host,
                port=server_config.port,
                reload=server_config.reload,
                reload_dirs=[str(project_root)],
                log_level="debug" if server_config.debug else "info"
            )
        else:
            # éçƒ­é‡è½½æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨åº”ç”¨å®ä¾‹
            uvicorn.run(
                app,
                host=server_config.host,
                port=server_config.port,
                reload=False,
                log_level="debug" if server_config.debug else "info"
            )
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    
    parser = argparse.ArgumentParser(description="MediaCrawler API æœåŠ¡å™¨")
    parser.add_argument("--host", type=str, help="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, help="æœåŠ¡å™¨ç«¯å£å·")
    parser.add_argument("--no-reload", action="store_true", help="ç¦ç”¨çƒ­é‡è½½")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # å°†å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼Œç¡®ä¿çƒ­é‡è½½æ—¶é…ç½®ä¸ä¸¢å¤±
    if args.host:
        os.environ["API_HOST"] = args.host
    if args.port:
        os.environ["API_PORT"] = str(args.port)
    if args.no_reload:
        os.environ["API_RELOAD"] = "false"
    if args.debug:
        os.environ["API_DEBUG"] = "true"
    
    # é‡æ–°åˆ›å»ºé…ç½®å®ä¾‹ä»¥è¯»å–ç¯å¢ƒå˜é‡
    global server_config
    server_config = ServerConfig()
    
    # å¯åŠ¨æœåŠ¡å™¨
    start_server(
        host=args.host,
        port=args.port,
        reload=not args.no_reload if args.no_reload else None,
        debug=args.debug if args.debug else None
    )

if __name__ == "__main__":
    # è¿è¡ŒæœåŠ¡å™¨
    main()